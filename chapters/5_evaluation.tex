% Measurement results / analysis / discussion:
% whatever you have done, you must comment it, compare it to other systems, evaluate it
% usually, adequate graphs help to show the benefits of your approach
% caution: each result/graph must be discussed! whatâ€™s the reason for this peak or why have you ovserved this effect

\chapter{Evaluation}\label{ch:evaluation}\glsresetall
TBW
\section{Experimental Setup}
Evaluation of our system solely depends on how the deep learning mode perform with the datasets we generated. We retrieved real-time data from $138$ different markets over a period of $33$ days, and the pairing coin of each market is Bitcoin. The data retriever fetched data in a fixed interval of $5$ seconds, resulting approximately $\numprint{570 000}$ samples per market. In total we collected \SI{47}{\giga\byte} of data. We processed the data, and the time lag we choose when calulcating the percentage of change was $10$ minutes, or approximaltely 120 points.

% 280 anomalies
% 200 pd

The computer we used to train our model with has the following scpecifications:
\begin{itemize}
    \item CPU - Intel Xeon E5-1620 \SI{3.9}{\giga\hertz}
    \item RAM - \SI{64}{\mebi\byte} DDR3 
    \item GPU - Nvidia GeForce GTX 770
\end{itemize}

We used the anomaly detection algorithm as we have previously outlined to detect anomalies from Binance, and filtered out the all anomalies that looked like a \ac{pd}. The following charts shows some of intervals that where labeled.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{true_1.pdf}
    \includegraphics[width=\textwidth]{true_2.pdf}
    \includegraphics[width=\textwidth]{true_3.pdf}
    \caption{Anomalies that seems like a \ac{pd}}
    \label{fig:label_true}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{false_1.pdf}
    \includegraphics[width=\textwidth]{false_2.pdf}
    \includegraphics[width=\textwidth]{false_3.pdf}
    \caption{Anomalies that not seems like a \ac{pd}}
    \label{fig:label_false}
\end{figure}

We choose to split our dataset into a training and test set, where the training set takes up $75\%$ ($104$ markets), and the test consists of the remaining $25\%$ ($33$ markets) of the dataset.  With the training dataset that we generated, we undersampled the dataset to create a blanaced dataset to trained a \ac{lstm} network with, where the hidden layer contain a single layer with $50$ \ac{lstm} cells, we also added dropout the \ac{lstm} cells, to prevent over-fitting as \ac{lstm} tends to often overfit their training data~\cite{overfit}. The output layer contained a single perceptron. We trained the network with balanced dataset in  $\numprint{5000}$ epochs. Also, we use the \emph{adam} optimizer, which is an extension to stochastic gradient descent~\cite{kingma2014adam}, the optimizer has shown that the model's weights will converge faster that results in great performance rapidly~\cite{adam}. To classify the performance of the model, we classified all samples in the test dataset.

\section{Results}
We evaluate our model by using several metrics, the first metric we use is a confusion matrix like \autoref{tab:confmat}. A confusion matrix shows the number of correct and incorrect classifies samples. \autoref{tab:model_performance} shows the results we obtained by using the parameters described. With the confusion matrix we can define a various performance measures as \autoref{tab:performance} illustrates.

\input{results/confusion_matrix.tex}
\input{results/model.tex}
\input{results/performance.tex}

From \autoref{tab:performance}, we can see that the model itself achieve a respective accuracy, in total it achieve around $98\%$ accuracy. The true positive rate is approximately $90\%$ and true negative ratio is circa $98\%$. These ratios is in general are good, if and only if the problem itself is hard to solve and if the dataset is balanced. Detecting \ac{pd} is a hard problem, it is hard even for us to identify \acp{pd}, there was multiple occasions during the filtering phase where we had problems with distinguishing \acp{pd}. Also, considering the type of data we had to work with, and how few \acp{pd} samples we have compared to regular trading data, there was over $\numprint{2000}$ times more samples that is categorized as negative than positive,....

In terms of \emph{specificity}, the model almost scores flawlessly, $99.99\%$. Specificity is ratio of how often we not classifies a positive \ac{pd} as negative. From the \autoref{tab:confmat}, we see that the model classified $911$ positive points as negative, while we classified $\numprint{17933021}$ correctly negative. Hence, the model is very precise when it boils down to classify negative samples as negative.

\autoref{fig:roc} illustrate the \ac{roc} curve of our model, and it gives us a visual perspective of the performance of the model. Ideally, a classifier has a tp-rate of $1$ and an fp-rate of $0$, and hence a classifier is better the more its \ac{roc} curve gets closer to the upper-left corner. On the diagonal, we make as many true decisions as false ones, and this is the worst case~\cite[p.~563]{alpaydin2014introduction}. Our \ac{roc} curve also shows that our model has accurate predictable capabilities. The \ac{auc} score represents the probability that a randomly chosen positive example is correctly classified ranked with greater suspicion than a randomly chosen negative example~\cite{bradley1997use}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{results/roc.pdf}
    \caption{Roc Curve}
    \label{fig:roc}
\end{figure} 

Currently, some metrics clearly shows that the model has good predictable capabilities in terms of accuracy. Another metric that reveal  the weakness of our model, is the f-score. As the \ac{roc} curve focuses on the relationship between tp-rate and fp-rate, the f-score measure our model test's accuracy with the weight on the measures \emph{recall} and \emph{precision}. Precision is an important measure when it comes to imbalanced dataset because the precision says something regarding the accuracy of correctly classified positive predicted samples. Our model only achieve a disappointing precision on $2\%$, and that value clearly demonstrate that $2\%$ of the samples that we classify as \acp{pd} truly are \acp{pd}, and regarding the f-score. An ideal f-score is $1$, and the worst is $0$. This measure is flexible in terms that we can weigh both recall and precision differently by adjusting the parameter $\gamma$. Where the closer $\gamma$ is to $0$, the more we weigh precision, when $\gamma$ is $1$ we wight them equally, and finally when $\gamma$ is over $1$, we weigh recall most.
\input{results/f_score.tex}

The following results shows the confusion matrix of each classified market.

\input{results/cm.tex}
\section{Discussion}
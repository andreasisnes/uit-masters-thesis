\chapter{Evaluation}\label{ch:evaluation}\glsresetall
This chapter evaluates \project. It first describes how the experiment we did was executed. Then, it presents the result. Finally, it discusses the results and proposes potential improvements.

\section{Experimental Setup}\label{sec:experimental_setup}
We started this experiment by retrieving real-time data from $138$ different markets throughout $33$ days, where every markets' pairing coin was Bitcoin. The interval between the data points was $5$ seconds, which resulted in approximately $\numprint{570 000}$ samples per market. Over these days, we collected in total \SI{47}{\giga\byte} of data. The sources that we used to fetch data from were those we presented in the \autoref{ch:implementation}, namely Binance, CoinMarketCap, and aggregated data from multiple sources. 

\autoref{tab:feature_table} contains details regarding every feature that were fetched. The operation column describes how we processed features. We cleaned the data by removing features that we believed was unproductive and those were tagged as \emph{Removed}. The features tagged \emph{PoC}, was first interpolated and then calculated the percentage of change. We chose $10$ minutes for the time lag, due to the period where a \ac{pd} start to where it peak is around $10$ minutes as described in \autoref{ch:background}. The field \emph{Imbalance} and \emph{Time} are processed like described in \autoref{ch:design}.

\input{tables/features.tex}

We used the anomaly detection algorithm which we previously described in \autoref{ch:design} to pinpoint \ac{pd} intervals. We fetched \ac{ohlcv} values that span over the period we collected data, and these \ac{ohlcv} values had an interval of one hour. The threshold parameters we chose for the algorithm was a price increase of $1.10$ and a volume increase of $3.00$, and the time lag we chose was $6$ hour. By using this algorithm we were able to identify in total $280$ anomalies. Of these anomalies we removed $80$ that seemed false.

\autoref{fig:label_true} shows three \ac{pd} anomalies we believe was true, while \autoref{fig:label_false} shows three \ac{pd} anomalies we believe was false. In \autoref{fig:label_true} the price suddenly increases by approximately $20\%$. This sudden increase only lasts for a few minutes before the price dumps straight down to what it was before the increase. These price characteristics are precisely like the \ac{pd} patterns we previously defined in \autoref{tab:pd_characteristics}. 

\autoref{fig:label_false} on the other hand, show anomalies that we did not believe was \acp{pd}. And it seems like the price fluctuates substantially, and the price does indeed fluctuate, but the scale in these charts is different from the other. The difference between the lowest and the highest price was around $5\%$ in these charts. If any of these allegedly false \acp{pd} was true, then these fail in raising the price significantly and suddenly.

\if0
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{true_1.pdf}
    \includegraphics[width=\textwidth]{true_2.pdf}
    \includegraphics[width=\textwidth]{true_3.pdf}
    \caption{Anomalies that seemed like a \ac{pd}}
    \label{fig:label_true}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{false_1.pdf}
    \includegraphics[width=\textwidth]{false_2.pdf}
    \includegraphics[width=\textwidth]{false_3.pdf}
    \caption{Anomalies that not seemed like a \ac{pd}.}
    \label{fig:label_false}
\end{figure}
\fi

\begin{figure}[hbt!]
    \centering
    \begin{subfigure}{.49\textwidth}
        \centering
        \begin{subfigure}{\textwidth}
            \includegraphics[trim={3.6cm 1.3cm 2.95cm 1.3cm},clip,width=\textwidth]{true_1.pdf}
            \caption*{SNT-BTC}
        \end{subfigure}
        \begin{subfigure}{\textwidth}
            \includegraphics[trim={3.6cm 1.3cm 2.95cm 1.3cm},clip,width=\textwidth]{true_2.pdf}
            \caption*{AST-BTC}
        \end{subfigure}
        \begin{subfigure}{\textwidth}
            \includegraphics[trim={3.6cm 1.3cm 2.95cm 1.3cm},clip,width=\textwidth]{true_3.pdf}
            \caption*{EDO-BTC}
        \end{subfigure}
        \caption{Anomalies that seemed like a \ac{pd}.}
        \label{fig:label_true}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\textwidth}
        \centering
        \begin{subfigure}{\textwidth}
            \includegraphics[trim={3.6cm 1.3cm 2.95cm 1.3cm},clip,width=\textwidth]{false_1.pdf}
            \caption*{MDA-BTC}
        \end{subfigure}
        \begin{subfigure}{\textwidth}
            \includegraphics[trim={3.6cm 1.3cm 2.95cm 1.3cm},clip,width=\textwidth]{false_2.pdf}
            \caption*{EOS-BTC}
        \end{subfigure}
        \begin{subfigure}{\textwidth}
            \includegraphics[trim={3.6cm 1.3cm 2.95cm 1.3cm},clip,width=\textwidth]{false_3.pdf}
            \caption*{STEEM-BTC}
        \end{subfigure}
        \caption{Anomalies that not seemed like a \ac{pd}.}
        \label{fig:label_false}    
    \end{subfigure}
    \caption[Anomalies in the dataset]{These anomalies are only $6$ out of $280$ anomalies that were collected. The three anomalies on the right were later removed as those were believed to be false, while the three on the left was kept as they seemed legitimate.}
\end{figure}

To create a dataset, we used the anomalies to label the collected data. The generated dataset was then normalized by using the min-max normalization method. We split the dataset into a training set and a test set. The training set consisted of $75\%$ of the dataset, which resulted in data from $104$ markets, while the test set contained the remained $25\%$ of the markets, which resulted in $33$ markets. The training dataset was undersampled in order to create a balanced dataset.

The model we used was a \ac{lstm} network, where its hidden layer contained a single layer with $50$ \ac{lstm} cells, where each cell had a time lag (short memory) of $10$ points, which resulted in $50$ seconds of memory as the interval between each sample is $5$ seconds. We also added dropout to prevent over-fitting as \ac{lstm} cells tend to frequently overfit their training data~\cite{overfit}. The output layer contained a single perceptron. The optimizer we used was \emph{adam}, which is an extension to stochastic gradient descent~\cite{kingma2014adam}, the optimizer has shown that a model's weights converges faster~\cite{adam}. The network was trained in over $5000$ epochs with the training dataset, where the batch size was $10$. To define the performance of the network, we classified all the samples in the test dataset and rounded the probabilistic prediction to either $0$ (\ac{pd}) or $1$ (not \ac{pd}). 

The computer we used to train our model with had the following specifications:
\begin{itemize}
    \item CPU - Intel Xeon E5-1620 \SI{3.9}{\giga\hertz}
    \item RAM - \SI{64}{\gibi\byte} DDR3 
    \item GPU - Nvidia GeForce GTX 770
\end{itemize}
%\newpage

\section{Results}
We use a confusion matrix as the first metric to evaluate the performance of our model. A confusion matrix is structured like \autoref{tab:confmat} and it shows the number of correct and incorrect classified samples, and it helps us defining further metrics. True positive, $tp$ are the number of samples that are correctly classified as \ac{pd}, while true negative, $tn$ is the number of  samples that are correctly classified as not \ac{pd}. $fp$ are samples that are incorrect classified as \ac{pd}, and false negatives, $fn$ are samples that are \ac{pd}, but classified as not \ac{pd}. $p$ and $n$ are the true total numbers of samples in each class, while $p'$ and $n'$ are the total numbers of predicted samples in each class. Finally, $N$ is the total number of samples.

\input{results/confusion_matrix.tex}
\input{results/model.tex}

As we see in \autoref{tab:model_performance}, the test dataset, as well as the training dataset is greatly imbalanced. Only $\numprint{8821}$ samples are \acp{pd}, while $\numprint{18321816}$ samples are normal trading data. In our test dataset, over one of every $\numprint{2000}$ sample was positive! \autoref{tab:model_performance} contains the aggregated results of every market we tested the model on. The confusion matrix of each market will be presented later in this section. It is more feasible to present our model's prediction capabilities by using a single matrix, instead of undergoing each market's confusion matrix.

\input{results/performance.tex}

From \autoref{tab:performance}, we see various metrics that we can use when defining the performance of our model, the metrics \emph{accuracy} and \emph{error}, are contrary measures. The accuracy, is the percentage of how many samples we classified correctly, and the error is the percentage of how many samples we misclassified, which is $97.82\%$ and $2.17\%$ respectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{results/roc.pdf}
    \caption[\project's \acl{roc} curve]{\acl{roc} describes our classifier performance using a curve. A classifier is better the more its curve gets closer to the upper-left corner. And on the diagonal, it makes as many true decisions as false ones. The \acl{auc} represents the probability that a random chosen positive }
    \label{fig:roc}
\end{figure} 

The tp-rate is the ratio of correctly classified \ac{pd} samples. When given a positive \ac{pd} sample, then there is a $89.67\%$ chance of classifying it correctly. The fp-rate is similar, when given a negative sample, Then there is a $2.12\%$ chance of classifying it positive. These two measures are visualizable using a \ac{roc} curve, as illustrated in \autoref{fig:roc}. It gives us a visual perspective of the performance of the model. Ideally, a classifier has a tp-rate of $1$ and an fp-rate of $0$, and a classifier is better the more its curve gets closer to the upper-left corner. And the closer the curve is to the diagonal, we make as many true decisions as false ones, and this is the worst case ~\cite[p.~563]{alpaydin2014introduction}. The \ac{auc} score is the rate of successful classifications~\cite{bradley1997use}. Our \ac{roc} curve and \ac{auc} shows that our model has accurate predictable capabilities.

The \emph{sensitivity} and \emph{specificity} is the ratio of classifying a positive or negative sample correctly. The sensitivity is the percentage of classifying a positive \ac{pd} sample correctly, which is $89.67\%$. The specificity is the opposite and is the percentage of classifying a negative sample correctly, which is $97.87\%$.

The metrics shown until now proves that the model has acceptable predictable capabilities. However, the metrics that reveal the flaws of our model are the measures \emph{recall} and \emph{precision}. Recall is precisely like the tp-rate and sensitivity, and is the percentage of classifying a positive sample correctly. Precision on the other hand, is the percentage of samples that are classified as \ac{pd} and are correct. In our case, a disappointing $1.99\%$. So whenever our model predicts a \ac{pd}, then there is only a $1.99\%$ that it is correct, and this results in plenty of false alarms.

\input{results/f_score.tex}

The \emph{F-score} is a common metric that relies on recall and precision. This formula is in \autoref{fig:f_score}. An ideal F-score is $1$, and the worst is $0$. This measure is flexible in terms that we can emphasize recall and precision differently by adjusting the parameter $\gamma$. When $\gamma$ gets closer to $0$, the more we emphasize precision, and when $\gamma$ is $1$ we emphasize them equally, and finally when $\gamma$ is over $1$, we emphasize recall most. And as we see, when emphasizing precision most by giving  $\gamma=0.5$, then the score yields only $0.0247$. When equally emphasizing them by giving $\gamma=1$, the score increase slightly, and finally, when emphasizing recall most the score is $0.0912$.

\autoref{tab:results} shows the confusion matrix of each classified market. We colorize the cells to visualize the performance of the model. The green cells on the diagonal contain the number of correctly classified samples, and the color intensity illustrates the percentage of correctly classified samples in its class. The red cell on the anti-diagonal consist of the number of misclassified samples and the color intensity represents the percentage of misclassified samples in its class. The intensity in white cells has a split meaning, it is a good sign on the anti-diagonal (red), but an awful one on the diagonal (green).

\input{results/cm.tex}

\newpage
\section{Discussion}
To interpret the results reasonable, we need to define the potential consequences of misclassifications. So, to push it to the extreme, assume our real job was to classify whether a patient has cancer using \ac{ml}. The model we use to predict cancer has already be trained and does have accurate predictions, but like many other models, it is not entirely flawless, and occasionally it makes an incorrect prognosis of cancer. At some point in the future, the model makes a wrong decision, and it predicts that a patient has cancer, while the patient is actually cancer free. The consequences are that the patient is starting a treatment, it can also have other side effects that impact her or him in a somatic and psychological way, but the patient will continue to live. If we turn the table, and the model predicts that a patient does not have cancer, while the patient truly has. Then, at some point in the future, the patient will die of cancer, which is an unforgivable fault made by us. This example only clarifies the potential harm of misclassifications, and it is not like that any will die from misclassifying \acp{pd}.

So to apply the consequence of misclassifications in the detection of \acp{pd}. Assume that, an exchange uses our model to detect \acp{pd} in order to ban users that participate in them. As we have seen, our model is not perfect, it makes wrong decisions occasionally, and at some point in the future, our model makes a wrong decision and incorrectly predicts a \ac{pd}. Subsequently, the exchange bans a set of investors from using their platform. And by banning investors on false terms, will undermine the exchange's credibility, and have many additional long-term consequences for them. On the other hand, missing \acp{pd} are not harmful. Despite that, some investors avoid getting banned, but they might not be so lucky if they ever try again.

As we saw from our results, all the metrics except the F-score and precision performs excellent and excels other models that detect \acp{pd}. However, The F-score and precision illustrate a fatal flaw in our model, and as the model currently is, deploying it now will be remarkably hard as there would be too many false alarms. There are ways to adjust the precision but at a particular cost. Improving it will with a high likelihood cost us our current tp-rate and make that worse, but due to the nature of the metrics, increasing precision will also increase other measures. As changing the precision means, either increase the true positives or decrease false positives.

Increasing true positives in our case can make precision worse, as we must try to make the model fit to additional positives samples simultaneously as there are over $\numprint{2000}$ more negative samples. So, our angle should be to decrease the false positives, in order to increase precision, but that may result in a decrease of true positives. 

To adjust the false positives, we can be more strict when classifying, as mentioned in end of \autoref{sec:experimental_setup}, we rounded each prediction, probabilities over $0.5$ is classified as negative, while probabilities under $0.5$ are classified as positive. If the prediction is precisely on the $0.5$ threshold, then the model can not separate it, but this case is scarce, and a case that is often entirely ignore. However, the point is that we can adjust this $0.5$ threshold in terms of strict we want to be. By lowering this threshold, we can be more strict; we can be more "sure" of whether it genuinely is a \ac{pd} or not. This method is also very flexible as the threshold is simple to adjust and do not require us to train our model again.

Other methods to adjust false positives is, with a purpose, to train our model with an imbalanced dataset that inclines the negative class. This method is not preferred as it requires us to retrain our model over and over again to evaluate it. This method was also our first trial, which when evaluating resulted in an astonishing $99.99\%$. However, there was only one problem. It could not recognize any \acp{pd}; it classified all samples as negative.

We also tried to adjust the loss-cost of each class during training, such that it favored positive over negative. This method was error-prone and incredibly hard to control as we had to guess how much our model's weights should change during training.

Ultimately, before using \project, one should consider the various consequences of misclassifications and then adjust the decision threshold accordingly to minimize misclassifications in one class.
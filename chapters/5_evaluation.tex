% Measurement results / analysis / discussion:
% whatever you have done, you must comment it, compare it to other systems, evaluate it
% usually, adequate graphs help to show the benefits of your approach
% caution: each result/graph must be discussed! whatâ€™s the reason for this peak or why have you ovserved this effect

\chapter{Evaluation}\label{ch:evaluation}\glsresetall
\section{Experimental Setup}\label{sec:experimental_setup}
To evaluate \project, we investigate its model prediction capabilities, and we did so by retrieving real-time data from $138$ different markets throughout $33$ days. The pairing coin of each market was Bitcoin. The interval between each data point was $5$ seconds, resulting in approximately $\numprint{570 000}$ samples per market. Over these days, we collected in total \SI{47}{\giga\byte} of data. The sources that we use to fetch data from was those we presented in the \autoref{ch:implementation}, namely, Binance, CoinMarketCap, and aggregated data from multiple sources. 

\autoref{tab:feature_table} contains details regarding every feature that were fetched. The operation fields describe how we decided to process a specific feature. The data were cleaned by removing features that we believe was unproductive in the detection of \ac{pd}, these were tagged as \emph{Remove}. The field that contains \emph{PoC}, was first interpolated and then calculated the percentage of change, and the time lag we chose was $10$ minutes because the time from where a \ac{pd} start, to where it peak is $10$ minutes, like described in \autoref{ch:background}. The field \emph{Imbalance} and \emph{Time} are the order book imbalance and how close the timestamp is on the hour.

\input{tables/features.tex}

We used the anomaly detection algorithm which we have previously described to pinpoint \ac{pd} intervals. We fetched \ac{ohlc} values that span over the period we collected data, and these \ac{ohlc} values had an interval of $1$ hour. The threshold parameters we chose for the algorithm is a price increase of $1.10$ and a volume of increase threshold of $3.00$, and the time lag we chose was $6$ hour. By using this algorithm we were able to identify in total $280$ anomalies. Of these anomalies we manually removed $80$, which we believe was false. \autoref{fig:label_true} shows three \acp{pd} charts we believe was true, while \autoref{fig:label_false} shows three \ac{pd} charts we believe was false.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{true_1.pdf}
    \includegraphics[width=\textwidth]{true_2.pdf}
    \includegraphics[width=\textwidth]{true_3.pdf}
    \caption{Anomalies that seems like a \ac{pd}}
    \label{fig:label_true}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{false_1.pdf}
    \includegraphics[width=\textwidth]{false_2.pdf}
    \includegraphics[width=\textwidth]{false_3.pdf}
    \caption{Anomalies that not seems like a \ac{pd}}
    \label{fig:label_false}
\end{figure}

To create a dataset, we used the anomalies to label all the processed, collected data. The generated dataset was then normalized by using the min-max normalization method. We split the dataset into a training set and a test set. The training set was consist of $75\%$ of the dataset, resulting in data from approximately $104$ markets, while the test set contains the remaining $25\%$ of the markets, resulting in $33$ markets. The training dataset was undersampled in order to create a balanced dataset.

The network we used was a \ac{lstm} network, where its hidden layer contains a single layer with $50$ \ac{lstm} cells, where each cell had a "short memory" of $10$ points, which results in $50$ seconds of memory considering that interval between each sample is $5$ seconds. We also added dropout to prevent over-fitting as \ac{lstm} cells tend to overfit their training data often~\cite{overfit}. The output layer contained a single perceptron. The optimizer we used was \emph{adam}, which is an extension to stochastic gradient descent~\cite{kingma2014adam}, the optimizer has shown that the model's weights will converge faster that results in greater performance rapidly~\cite{adam}. The network was trained in over $5000$ epochs with the training dataset, where the batch size was $10$. To define the performance of the network, we classified all the samples in the test dataset and rounded the probabilistic prediction to either $0$ (\ac{pd}) or $1$ (not \ac{pd}). 

The computer we used to train our model with had the following specifications:
\begin{itemize}
    \item CPU - Intel Xeon E5-1620 \SI{3.9}{\giga\hertz}
    \item RAM - \SI{64}{\gibi\byte} DDR3 
    \item GPU - Nvidia GeForce GTX 770
\end{itemize}
\newpage

\section{Results}
The first metric we use is a confusion matrix, which is structured like \autoref{tab:confmat}. A confusion matrix shows the number of correct and incorrect classified samples, and help define further scores. True positive $tp$ are samples that are correctly classified as \ac{pd}, while true negative, $tn$, is samples that are correctly classified as not \ac{pd}. $fp$ are samples that are incorrect classified as \ac{pd}, and false negatives, $fn$, are samples that are \ac{pd}, but classified as not \ac{pd}. $p$ and $n$ are the true total numbers of samples in each class, while $p'$ and $n'$ are the total numbers of predicted samples in each class. Finally, $N$ is the total number of samples.

\input{results/confusion_matrix.tex}
\input{results/model.tex}

As we can see from \autoref{tab:model_performance}, the test dataset, as well as the training dataset is hugely imbalanced. Only $\numprint{8821}$ samples are \ac{pd}, while $\numprint{18321816}$ is not a \ac{pd}. Over one of every $\numprint{2000}$ sample is a \ac{pd}!

\input{results/performance.tex}

From \autoref{tab:performance}, we see various metrics that we can use when defining the capabilities of the \ac{lstm} network, the metrics \emph{accuracy} and \emph{error}, are measures that are opposite of each other. The accuracy is the percentage of how many samples we classified correct. The error, says how many samples we misclassified. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{results/roc.pdf}
    \caption{Roc Curve}
    \label{fig:roc}
\end{figure} 

The tp-rate is percentage of how often we correctly classify \ac{pd} samples. So given a \ac{pd} sample, there an $89.67\%$ chance of classifying it correctly. The fp-rate is similar in terms of, given a negative sample. Then there is a $2.12\%$ change of classifying it at as a \acp{pd}. These measures can also be visualized through \ac{roc} curve illustrated in \autoref{fig:roc}. It gives us a visual perspective of the performance of the model. Ideally, a classifier has a tp-rate of $1$ and an fp-rate of $0$, and a classifier is better the more its \ac{roc} curve gets closer to the upper-left corner. On the diagonal, we make as many true decisions as false ones, and this is the worst case~\cite[p.~563]{alpaydin2014introduction}. Our \ac{roc} curve also shows that our model has accurate predictable capabilities. The \ac{auc} score represents the probability that a randomly chosen positive example is correctly classified ranked with greater suspicion than a randomly chosen negative example~\cite{bradley1997use}.
The \emph{sensitivity} and \emph{specificity} balance the classes, they tells us, given a negative or positive samples, what is the percentage of classifying it correctly. The sensitivity, is the percentage of given a positive sample, the percentage of classifying it correctly, is $89.67\%$. The specificity, is the opposite, given a negative sample, the percentage of classifying it correctly is $97.87\%$. 

Clearly, the metrics we presented now shows that the model has good predictable capabilities. However, the metrics that reveal the flaws of the model, are the measures \emph{recall} and \emph{precision}. Recall are exactly like the tp-rate and sensitivity, it is the percentage of, given a \ac{pd} sample, then there is $89.67\%$ chance of classifying it correctly. \emph{precision} on the other hand, is, if classifying a sample as a \ac{pd}, then, what is the percentage that the sample truly is a \ac{pd}. In our case, a disappointing $1.99\%$. so whenever our model predicts a \ac{pd}, then there is only a $1.99\%$ that it is correct, and this results in plenty of false alarms.

\input{results/f_score.tex}

A frequently used score that relies on recall and precision is the \emph{f-score}. The f-score formula is showed in \autoref{fig:f_score}, and an ideal f-score is $1$, and the worst is $0$. This measure is flexible in terms that we can weigh both recall and precision differently by adjusting the parameter $\gamma$. Where the closer $\gamma$ is to $0$, the more we weigh precision, when $\gamma$ is $1$ we wight them equally, and finally when $\gamma$ is over $1$, we weigh recall most. As we see, when weighing precision most, by giving  $\gamma=0.5$, the score only yield $0.0247$. When equally weighing them, by giving $\gamma=1$, the score increase slightly. Finally, when weighting recall most, the score is $0.0912$, which is better than the other.

The following results shows the confusion matrix of each classified market.

\input{results/cm.tex}

\newpage
\section{Discussion}
To interpret the results reasonable, we need to define the potential consequences of a misclassification. So, to push it to the extreme, assume our real job was to classify whether a patient has cancer using machine learning. The model we use to predict cancer has already be trained and does have accurate predictions, but like many other models, it is not entirely flawless, and occasionally it will make an incorrect prognosis of cancer. At some point, the model makes a wrong decision, and it predicts that a patient has cancer, while the patient is actually cancer free. The consequences might be that the patient is starting a treatment, it can also have other side effects that impact her or him in a somatic and psychological way, but the patient will at least live. If we turn the table, and the model predicts that a patient does not have cancer, while the patient truly has. Then, in short amount of time, the patient will die of cancer, which is an unforgivable fault made by us. This example only clarifies the potential harm of misclassifications, and it is not like that any will die from misclassifying \acp{pd}, hopefully.

So to apply the consequence of misclassifications in the detection of \acp{pd}. Assume that, an exchange uses our model to detect \acp{pd} in order to ban users that participate in them. As we have seen, our model is not perfect, it makes wrong decisions occasionally, and just like Murphy's law, everything that can happen will happen. At some point, our model makes a wrong decision and predicts a \ac{pd} and the exchange ban a set of investors from using their platform. Banning investors on false terms, will undermine the exchange's credibility, and have various additional consequences for them. On the other hand, missing \acp{pd} are not harmful. Despite that some investors avoid getting banned, but if the investors ever try again, they might not be so lucky.

As we saw from our results, all the metrics except f-score and precision performs excellent and excels other models that detect \acp{pd}. The f-score and precision is a fatal flaw of our model, and as the model currently is, deploying it now will be remarkably hard as there would be too many false alarms. However, there are ways to adjust the precision, but at a particular cost. Improving it will with a high likelihood cost us our current tp-rate and make that worse, but due to the nature of the metrics, increasing precision will also increase some other measures. As changing the precision means, either increase the true positives or decrease false positives.

Increasing true positives in our case can make precision worse, as we must try to make the model fit to additional positives samples simultaneously as there are over $\numprint{2000}$ more negative samples. So, our angle should be to decrease the false positives, in order to increase precision, but that may result in a decrease of true positives. 

To adjust the false positives, we can be more strict in terms when classifying, as mentioned in end of \autoref{sec:experimental_setup}, we rounded each prediction, probabilities over $0.5$ is classified as negative, while probabilities under $0.5$ is classified as positive. If the prediction is precisely on the $0.5$ threshold, then the model can not separate it, but this case is scarce, and a case we often entirely ignore. However, the point is that we can adjust this $0.5$ threshold in terms of strict we want to be. By lowering this threshold, we can be more strict, we can be more "sure" of whether it genuinely is a \ac{pd} or not. This method also very flexible as the threshold is simple to adjust and do not require us to train our network again.

Other methods to adjust false positives is, with a purpose, to train a network with an imbalanced dataset that inclines the negative class. However, this method is not preferred as it requires to retrain the model over and over again to evaluate it. This method was also our first trial, which, when evaluating it resulted in an astonishing $99.99\%$. However, there was only one problem, it could not recognize any \acp{pd}, it classified all samples as negative.

We also tried to adjust the loss cost of each the classes during training, such that it favored positive over negative. So when making a wrong decision of a \ac{pd}, the model's weight will adjust more then it would when making a wrong decision on the other way around. Ultimately, this method was error-prone and incredibly hard to control as we had to guess how much the weights should change.

Ultimately, before using \project, one should weigh the various consequences of a misclassification and adjust the decision threshold accordingly. 
\chapter{\project's Design}\label{ch:design}\glsresetall
This chapter presents the design of \project. First, it briefly describes the system and all the components within it. Then, it goes further into details regarding every component in terms of their structure, function, and intent. Finally, it describes the two phases \project has.

\section{Overview}
We are modeling \project as a \ac{ml} pipeline, but with some few extensions. \project is designed to extract raw data from different cryptocurrency sources and transform it into valuable features in order to classify \acp{pd}. The term \ac{ml} pipeline can be misleading as it implies a one-way flow of data when some elements in the pipeline are cyclical and iterative where every iteration intends to improve the accuracy of the model~\cite{ml_pipeline_3}. 
%Some of the advantages a pipeline provides is~\cite{ml_pipeline_2};

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{overview.pdf}
    \caption[\project's architecture]{Architectural overview of \project}
    \label{fig:overview}
\end{figure}

The first step in the pipeline pulls data over the internet from sources that has information regarding \acp{pd} in cryptocurrencies. The data is mostly incomplete by lacking trends or being unprocessed, making it potentially challenging to train a model and obtain good results. This process is tedious because sources tend to have different request rates, \acp{api}, and the data can have various formats like \ac{json} or \ac{xml}.

The next step branches the retrieved data in two, live and historical data. Since we are trying to detect \acp{pd} in real-time we need to store live data continuously. When we have captured a compelling amount of \acp{pd}, we use an anomaly detection algorithm~\cite{P&D_to_the_moon} to detect \acp{pd} in the gathered live data. This algorithm is not compliant with live data, so we need to pull aggregated historical data that span throughout the collected live data. As previously mentioned, anomaly detection algorithms tend to have a high false positive rate. Thus, we need to remove these false positives and keep the true positive manually.

The input-data ultimately determine the performance of a \ac{ml} deep learning model~\cite{mike_voets}. Training a model with the raw gathered live data is ineffective. Hence, we need to define a new convenient dataset containing features created by processing the collected live data; this is a highly critical process and will later determine the classification performance of the deep learning model. The gathered live data also need to go through a cleansing process as it most likely does not contains relevant \ac{pd} information.

With filtered anomalies containing \ac{pd}, we can create a labeled dataset and train our model. Obtaining good classification results depends, as mentioned, on the features, but also how we decide to preprocess the dataset. Typical preprocessing strategies include dimensionality reduction and normalization. Having a labeled preprocessed dataset we can finally begin to train the deep learning model. This a cyclical and a long process, as it requires many trial and error attempts to find the optimal weights for the model. In each cycle, we store the model's weights because it is not always the case that each iteration will improve the classification performance of the model.

For applications to utilize \project, they need to select a model and let live data flow through the same processing stages as the dataset that was used to train the model.

\section{Internal Components}
TBW

\subsection{Retrieving Data}
Every problem that is solved using \ac{ml} requires data. The more data, the better the results will be when training a model. As previously mentioned, cryptocurrency sources like exchanges produce time series data containing, e.g., price and volume of a coin. The data is continuously produced in a restricted amount with proportion to time.

Since we want to detect \acp{pd} on exchanges in real-time, the nature of the data we want to make classifications on is live fine-grained data so that the model can detect them as early and accurately as possible. Aggregated historical data is too coarse-grained because exchanges generally only allow a discrete time interval selection of data where the smallest is typically one minute. The duration where they start to where they peak varies from a few seconds to a maximum of ten minutes~\cite{P&D_MIT_crypto, P&D_to_the_moon}, and the ability to make accurate predictions with one-minute data is questionable.

Training a model in real-time by pulling data is impractical because it will not be labeled. In addition, sources can only produce a limited amount of data at a time which will create a bottleneck of data supply to the model. To cope with these problems, we have to pull and store current live data continuously; this is a time consuming process for we have to wait until we have captured enough \ac{pd} events before we can start training. If anything fails, we may have to start all over again as we are missing out on trends, which results in noisy data.

From the reinforcers field in \autoref{tab:pd_indicators}, we see that we have to fetch data from various sources. An exchange alone does not produce data regarding a coin's capitalization, nor a coin's price on a different exchange. Sources other than exchanges produce such metadata of coins, while exchanges only produce internal trading data.

\subsubsection{Master Slave Approach}
We shape our data retriever like a master/slave model. \autoref{fig:ms} is an example that illustrates our data retriever with a master and its three data-pulling slaves. Each slave in our data retriever is assigned a source that can e.g. be an exchange. The communication between slaves and the master is as follows. The master broadcasts a pull signal to the slaves, and they pull the data from their assigned source. Then the master gathers the data from them and parses it, and augments it to a sample. Each sample the master generate gets stored. This procedure takes effect in a fixed interval for obtaining. By letting the master signalize the slaves to retrieve data simultaneously, we collect clean time series data where each sample's time gap circa equal. 

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\textwidth]{ms.pdf}
    \caption{\project's data retriever}
    \label{fig:ms}
\end{figure}

\subsubsection{Collecting Trading Data From Multiple Markets}
Previous work in detection of \acp{pd}~\cite{P&D_to_the_moon} estimated that $1.6$ \acp{pd} is carried out daily per market, and this raises several problems. First, multiple exchanges have the same market, and we can not know which exchange they target unless we have prior knowledge from their \ac{pd} group. Second, gathering data from a single market is inadequate, the data retriever would with an estimate capture $48$ \ac{pd} occurrences, if it gathered every \ac{pd} event on a single market for a whole month. Training a model with $48$ \ac{pd} instances is highly insufficient. 

To alleviate this problem, we collect data for all markets from a single exchange to make sure we obtain as many \ac{pd} events as possible. We assume the \ac{pd} pattern remain the same across markets. Otherwise, we may run into trouble when training our model by having too few samples with various patterns. Also, by training the model using data from all the markets generalizes the model, which makes the model compliant to other new markets who are excluded during training.

%The reason we are targeting a single exchange is that they have different \acp{api}, although the data they provide are mostly identical, in some way, each exchange does stand out by providing some unique data. Thus, we can utilize every piece of information we receive from an exchange without the concern of making it compatible with data existing on other exchanges.

\subsubsection{Feature Description}
From the \ac{pd} indicators described in \autoref{tab:pd_indicators}, we define a set of features in \autoref{tab:features}. We believe that these features contains the necessary information for a model to detect \acp{pd}. We fetch all these features for each market.

Some features like a coin's capitalization are available at \href{https://coinmarketcap.com/}{CoinMarketCap}, while trading data like order book and \ac{ohlcv} values are available at exchanges. A specific feature that is challenging to attain is aggregated \ac{ohlc} values from multiple exchanges, as this requires us to request multiple exchanges simultaneously and aggregate the data.

\input{tables/feature_discription.tex}

%\newpage
\subsection{Preparing Data}\label{sec:prep}
Data preparation is an important task. In practice, it has shown that data cleaning and preparation takes approximately $80\%$ of the total data engineering effort, as it requires solid domain knowledge of the subject. Data preparation comprises those techniques concerned with analyzing raw data to yield quality data. Some of the techniques are; data collecting, data integration, data transformation, data cleaning, data reduction, and data discretization~\cite{zhang2003data}.

For the reason that we fetch data from multiple markets to create a generalized model, we have to prepare the data and make it equally scaled. Markets are distinct and they have different trading price and volume. Making predictions with these raw numerical values that vary from market to market is nonviable, as the model looks at each feature as more or less of something. If a \ac{pd} occurs on a coin with a low cost, and the price is increasing with $300\%$. Despite the high increase and profit, the coin is still almost worthless compared to other expensive coins and the numerical price increase do not necessary need to be that high. On the other hand, if the price of an expensive coin increase with only $1\%$, the numerical value can still be high despite the low percentage increase. Since the model is generalized it's weighing each markets equally. So using e.g raw data like price is infeasible, as a low price change in an expensive market reflects a gigantic change in a cheap market. Thus, we must transform all these raw market specific values to that are general across markets.

\subsubsection{Data Cleansing}
Data cleaning, also called data \emph{cleansing} or \emph{scrubbing}, deals with detecting and removing errors, inconsistencies, and unproductive features from data in order to improve the quality of data~\cite{data_cleaning}. Fetched data from exchanges includes additional features not defined in \autoref{tab:features}. These features are removed as they adds nothing of value. Instead, they increase the number of dimensions and makes the data more complex.

Markets with little or no activity may have intervals containing \emph{zero-data}. For example, if no investors have bought or sold assets in a specific interval exchanges tend to set the trading values to zero. These zero-values create significant spikes in the trend which we must address; otherwise, they disrupt the data. Besides, having zero-data does not make any sense, if the price is recorded to be zero, it means that the coin is free which it is not.

We substitute every value that is zero by linearly interpolating each of them. Linear interpolation involves estimating a new value by connecting two adjacent known parameters with a straight line~\cite{interpolate}. These two known parameters are non-zero values that are adjacent on each side of the zero-value. Thus, we form \autoref{eq:interpolation} with the known parameters $(x_1, y_1)$ (previous non-zero value) and $(x_2, y_2)$ (next non-zero value), $y$ is the new value for some zero-value in point $x$.

\input{equations/interpolation.tex}

\subsubsection{Feature Engineering}
Feature engineering is the process of using domain knowledge of the data to create features that make \ac{ml} algorithms work. Feature engineering is fundamental to the application of machine learning and is both challenging and expensive, but when done correctly, it can result in wonders~\cite{feature_engin}.

\begin{displayquote}
    \begin{em}
        "Feature engineering is the art part of computer science" - Sergey Yurgenson
    \end{em}
\end{displayquote}
 
\subsubsection{Processing Order Book}
As previously mentioned in \autoref{sec:pump_groups}, \ac{pd} organizers invest in the market before the \ac{pd} without raising the price. We believe that the order book in said market oscillates before the pump, and especially during the pump. Therefore, we calculate an imbalance between sell and bids order; this is a multidimensional problem considering an order book contains both a list with prices along with its volume, as we saw in \autoref{tab:order_book}. \autoref{eq:imbalance} reduces this multidimensional problem to a single value with equal weight to price and volume. $p$ and $v$ is respectively the lists with prices and volumes, and the annotations $a$ and $b$ denotes asks and bids orders. If \autoref{eq:imbalance} yields a value between $(0,1)$, it emphasize the bidders, if it yields a value between $(1, \infty]$, then askers, and $1$ they are equally worth.

\input{equations/imbalance.tex}

\subsubsection{Processing Trading Data}
We process trading data such as price and volume by calculating the percentage of change, which makes each market comparable, and eliminate the need for the model to adjust to prices. For example, if ETH increase \$$10$ from a starting value of \$$100$, and ADA increase \$$5$ from a starting value of \$$50$, then both coins increase by $10$\%.

We define the function $pct$ to calculate the percentage of change. It calculates the percentage change in point $x$ concerning a previous value $v$ with a time lag $\gamma$. We consider $x$ and $v$ to be a single value like the price or volume, while $\gamma$ indicates moving backward in time from point $x$. We must be vigilant when using this technique, if the data contains values that are zero, we might perform a zero-division that can clutter with the data unpredictably. However, since we scrubbed the data first and removed zero-values, this should not be a concern.

\input{equations/pct.tex}

Differentiating and take the tangent line instead could be a solution, but since we are fetching data from multiple markets, the tangent would always favor expensive markets. Using the tangent would be illogical when \ac{pd} organizers tend to target low-cost markets.

\subsubsection{Processing Time}
According to \cite{P&D_anatomy}, the time is essential when classifying \acp{pd}, as they are typically executed at the hour (6:00, 7:00, etc.) because organizers usually does not choose a random time.

\input{equations/time.tex}

Data retrieved from sources are getting timestamped, and we can take advantage of these timestamps to check whether data was generated at the hour or not. The function $x_\delta$ transforms a Unix\footnote{unix timestamp is a way to track time as a running total of seconds.} timestamp to a value in the interval $[0, 1)$. The closer $x_\delta$ is to the margins, the closer the time is to the hour, but since $0$ and $1$ symbolizes the same, we have to process it further before we can use it as a valuable feature.

We define a Gaussian distribution function $t$ with the parameters $\mu=0.5$ and $\sigma=0.1$ which creates the graph illustrated in \autoref{fig:unixtime}. The graph shows when given $t$ a $x_\delta$ close to $0$ or $1$ (6:00, 7:00, etc.), $t$ returns a value close to $0$, but when given $0.5$ (6:30, 7:30, etc.) it returns a value close to $4$. $t$ demonstrate that we can separate data by how close it was generated to the hour. Also, we can always tweak $\sigma$ to adjust the width of the wave. Adjusting the wave allows us to be more strict, the wider the curve, the more strict.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{time.pdf}
    \caption[\project's time processing curve]{Gaussian distribution with the parameters $\sigma=0.1$ and $\mu=0.5$.}
    \label{fig:unixtime}
\end{figure}

\subsection{Collecting Pump-and-Dumps}
Manually collecting \ac{pd} events from chat applications to label our collected data, seems infeasible in the long run â€” even tough Livshits and Xu \cite{P&D_anatomy} hand-picked $220$ pump-events from July to November in 2018 from $358$ different Telegram groups to train a Random Forest. They still did miss out on plenty of other executed \ac{pd} schemes as there are numerous of other chatting applications and private organizers~\cite{blockonomi}. Also, searching for \ac{pd} events is a time-consuming process, and incorrect labeling occurs when we lack membership to all of \ac{pd} groups, which results in more mediocre prediction capacity~\cite{label_noise}. We also have to be aware if a \ac{pd} was successfully executed or not; labeling failed attempts as positive only contributes to label noise. Instead of manually collecting \acp{pd} from groups in chatting applications, we believe that it is possible to hand-pick \acp{pd} by \emph{reasoning abductively} with the help of an anomaly detection algorithm to pinpoint suspicious time intervals in historical data.

The anomaly detection algorithm identifies local \emph{contextual anomalies} based on fixed recent history called a \emph{sliding time window}. Contextual or conditional anomalies are abnormal data points within a specific context and are prevalent in linearly ordered data called \emph{sequence data}~\cite{anomaly_survey}. A sliding window is a period that stretches back in time (lag factor) from the present containing events at specified intervals. The event intervals can overlap with each other, or they can be disjunct. As events exceed the lag factor, they fall out of the sliding window, and they are no longer matching against the rules applied to the sliding window~\cite{redhat}. With a sliding window, we can compare values in a given period~\cite{P&D_to_the_moon}, contrary to using single values, which not yield much information alone in sequence data.

The anomaly detection algorithm is proposed by Kleinberg and Kamps \cite{P&D_to_the_moon}, which is inspired by previous research in \ac{dos} attacks~\cite{dos}. It is a threshold based technique to find a suspicious increase in price and volume of a coin. If the price and volume in a specific interval are higher than some threshold, then the interval is flagged anomalous and worth further investigation.

\subsubsection{Price Anomaly}
We compute the price anomaly threshold by a simple moving average $\mu_\gamma^p$ of \ac{ohlcv} values denoted $x$ with a lag factor $\gamma$ multiplied with a given percentage increase $\epsilon_p$. We consider $x$ and $\gamma$ as \ac{ohlcv} objects, and $x-\gamma$ indicates moving backwards in the sliding time window by a factor of $\gamma$~\cite{P&D_to_the_moon}. If the highest registered price in $x$'s period is greater than the computed threshold, we flag the period as anomalous.
\input{equations/price.tex}

\subsubsection{Volume Anomaly}
Calculating the volume anomaly threshold is almost identical to the above, we are only substituting $x_{closing}$ and $x_{high}$ with $x_{volume}$, which form the following expression.
\input{equations/volume.tex}

\subsubsection{Filtering Anomalies}
Anomaly detection algorithms, in overall, have a high false alarm rate~\cite{grill2017reducing}, which makes them difficult to use. Since we want to use these anomalies we collected to train a model with, it is, with high importance, to remove false \acp{pd}. Otherwise, training a model with false \acp{pd} will make the model perform worse, and results in a high occurrence of false positives which is already a problem. Therefore, we manually remove all the false positives anomalies, but removing them requires prior knowledge of \acp{pd}, which we do not have.

Since the anomaly detection algorithm checks whether a \ac{pd} occurred in a specific interval, and not exactly when. We visualize one-minute \ac{ohlc} from that interval in a chart, by plotting these values we can compare them to a few charts containing real \ac{pd}. Additionally, a market's capitalization, if the pairing coin is BTC, or how many exchanges that list the market, can be helpful when filtering \ac{pd}. E.g., if the anomaly detection algorithm flagged a suspicious interval in the ETH-BTC market, and it visually looks like a \ac{pd}, then it still would with a high likelihood not be a \ac{pd} because ETH is the coin with second highest capitalization~\cite{coinmarketcap_eth}, and that would break the pattern where the organizers target coin with low capitalization. Further helpful characteristics regarding \acp{pd} is described in \autoref{tab:pd_characteristics} and \autoref{tab:pd_indicators}.

\subsubsection{Labeling Pump-And-Dumps}
With the new features generated by the collected data and filtered anomalies containing \acp{pd}, we can label the new features and define a dataset, but first, the filtered anomalies still contains intervals where \acp{pd} occurred, and not exactly when it was executed. So, we need to precisely define where every \ac{pd} started end ended, before labeling.

Because the purpose of \acp{pd} is to raise the price of an asset, makes us believe that, where the price change is highest in an interval, that is where the \ac{pd} peaked. If we have the peak of a \ac{pd}, we can search from the peak and down the descending slopes on each side of it. Left side, pump. Right side, dump. From each side we search until the change in price is equal or smaller than $0$. Finally, when have defined the start and end of every \ac{pd}, we label all the new features positive where there are \acp{pd}, and negative otherwise.

\subsection{Deep Learning}
This section describes the model we use to detect \ac{pd}, and how we process data, trains the model, and which metrics we should use to evaluate the model. These steps are an iterative process, where each iteration tend to improve the model.

\subsubsection{Preprocessing}
Before training a model with the generated dataset, we transform it by \emph{normalizing} Since the features in our dataset have various scales, some percentages, some other numerical values, then, a common technique do is to \emph{normalize} the input data. Normalization creates new values from the dataset, but still maintain the general distribution and ratios in the source data while keeping values within a scale applied across all used features~\cite{normalize_data}. Also, normalization, in some cases, improves the performance of the model~\cite{normalize_google}, and accelerate convergence speed resulting in shorter training time~\cite{sola1997importance}.

There are several ways to normalize data, and which normalization technique one use may have an impact on the performance of the model. Since \acp{pd} are anomalies, some features like percentage change in price and volume will make them look like outliers in the data. A traditional method called \emph{min-max} normalization is repeatedly used in detection of outliers~\cite{campos2016evaluation, goldstein2016comparative}. This technique provides a linear transformation of the data~\cite{panda2014smoothing}, allowing us to keep the distance ratio, and it scales every value into the range $[0,1]$.

\input{equations/min-max.tex}

\autoref{eq:minmax} is the min-max normalization formula for a coefficient $x_{ij}$, where $x_j$ is a column in a dataset with row vectors.

\subsubsection{Model}
The basic model type we use to detect \ac{pd} is a \ac{rnn} \ac{lstm} network. This network contains \ac{lstm} cells in the hidden layer because we want the cells to have an internal state (memory) to remember the trend in data. We modeled the detection of \acp{pd} as \emph{binary classification} problem, \ac{pd} or not \ac{pd}. So, we only need a single perceptron in the output layer. \autoref{fig:lstm} illustrates the model.

We can build a more complex network with multiple layers containing \ac{lstm} and perceptron cells, but with a deeper network, the time it takes to train increases simultaneously as it is harder to find the optimal parameters of the network. 

\begin{figure}[ht]
    \centering
    \includegraphics{lstm.pdf}
    \caption[\project's deep learning model]{Structure of the model}
    \label{fig:lstm}
\end{figure}

\subsubsection{Training}
Before we start to train our model, we have to split the dataset into two sets, a training set, and a test set. The training set will we use to train the model, and the test will we use to evaluate the model. There is also a validation set that is used to fine-tune the hyperparameters during the training, but since there are so few \ac{pd} samples, we choose to avoid using a validation set and use those \ac{pd} samples we have to train our model and evaluate it.

As previously mentioned, \acp{pd} are anomalies, and that results in a significant class imbalance between positive and negative samples, which, when trained will make the model to overfit to the negative class and more or less ignore the positive class, it entirely depends on the distribution of them. Also, the academia is split concerning the definition, implication and possible solutions to this problem~\cite{tw_imbalance_2} making it currently ambiguous how to properly deal with it.

\begin{figure}[ht]
    \centering
    \includegraphics{oversampling.pdf}
    \caption[Oversampling dataset]{Oversampling (source\cite{tw_imbalance})}
    \label{fig:oversampling}
\end{figure}

We believe that we can solve our imbalance problem by using a technique called \emph{oversampling}, which \autoref{fig:oversampling} illustrates. Oversampling gathers all the samples from the minority class and duplicates them until the amount is approximately equal to the majority class. There is also a technique called \emph{undersampling}, that select random samples into a subset from the majority class until the size is equal to the number of samples in the minority class, but by using the oversampling technique, we can train our model with the whole dataset. With a balanced dataset, we can finally train our model.

Sometimes, depending on the size of the dataset and number of epochs, training a model takes a really long time. Thus, after each training, we store the model and all its weights and other internal parameters.

\subsubsection{Evaluation}
After the training phase, we evaluate the model by classifying all the samples in the test dataset. The metrics, \emph{precision}, \emph{tp-rate}, and \emph{fp-rate} are probably the most important metrics to optimize when it comes to detection of anomalies. Sometimes, finding the best parameters is done by multiple trials and errors attempts. Adjusting the parameters like number of, hidden layers, cells, and epochs, might improve said metrics. Also, shuffling samples may improve the model as we train with a few numbers of original \acp{pd} samples.

\section{Stitching it all together}
Till now, in this chapter, we have described each component in \project. As we mentioned in \autoref{ch:background}, when we have trained a model and are satisfied with how it performs, we can be fed the model with real-time data, and it will classify whether the given data is a \ac{pd} or not. This allows us to dismiss numerous components which are not needed anymore. They only need to be present during the training phase. So we can divide \project into two phases, a training phase, and a deployment phase. In the training phase, \project entirely depends on all the components. In the deployment phase, there are only a few necessary components.

\subsection{Deployment Phase}
In the deployment phase, we only need a handful component, these components are in \autoref{fig:pipe}. The first component, the retriever function precisely like described above, it fetched data and pushes it further down the pipeline, but it is essential that it fetches the same type of data from the same sources. The next two stages are different, as they need to remain a state from the training phase. In the feature engineering stage, we have to use the same parameters in the cleansing and feature engineering method. Otherwise, the model will classify data that is not what it trained with. Also, in the processing stage, it is highly essential that we normalize the input data with the precisely same parameters as we used with the training set. Else, the input data is scaled the inconsistent parameters, which with result in more misclassification.

The application needs to select a model that was generated during the training phase. With both a trained model and processed real-time data, the application can with ease predict in real-time, whether if the input data is a \ac{pd} or not.

\begin{figure}[h]
    \centering
    \includegraphics{pipe.pdf}
    \caption[\project's deployment pipeline]{Deployment pipeline}
    \label{fig:pipe}
\end{figure}